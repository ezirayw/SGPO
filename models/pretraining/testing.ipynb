{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMTokenizer:\n",
    "    def __init__(self, esm_model_name='esm2_t12_35M_UR50D', sequences=True):\n",
    "        self.model, self.alphabet = getattr(esm.pretrained, esm_model_name)()\n",
    "        self.batch_converter = self.alphabet.get_batch_converter()\n",
    "        self.padding_idx = self.alphabet.padding_idx\n",
    "\n",
    "    @property\n",
    "    def pad_id(self):\n",
    "        return self.padding_idx\n",
    "\n",
    "    def tokenize(self, seq_tuple):\n",
    "        # unpack tuple input\n",
    "        seq_str = seq_tuple[0] if isinstance(seq_tuple, tuple) else seq_tuple\n",
    "        _, _, tokens = self.batch_converter([(\"seq\", seq_str)])\n",
    "        return tokens[0].numpy()\n",
    "\n",
    "    def untokenize(self, tokenized_seq):\n",
    "        # tokenized_seq is a tensor or numpy array\n",
    "        if torch.is_tensor(tokenized_seq):\n",
    "            tokenized_seq = tokenized_seq.cpu().numpy()\n",
    "        tokens = [self.alphabet.get_tok(int(tok)) for tok in tokenized_seq if tok != self.padding_idx]\n",
    "        # Remove special tokens from untokenized output\n",
    "        tokens = [tok for tok in tokens if tok not in ('<cls>', '<eos>', '<pad>')]\n",
    "        return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ESMTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 20, 15,  6, 19, 18,  6, 14, 19,  6,  6, 16, 19,  7, 14,  9, 12,\n",
       "        4, 20,  6,  5,  4,  9,  9,  4,  9,  5,  5, 19,  9,  6, 12, 20, 15,\n",
       "       13,  9,  8, 18, 22, 15,  9, 18, 17, 13,  4,  4, 10, 13, 19,  5,  6,\n",
       "       10, 14, 11, 14,  4, 19, 18,  5, 10, 10,  4,  8,  9, 15, 19,  6,  5,\n",
       "       10,  7, 19,  4, 15, 10,  9, 13,  4,  4, 21, 11,  6,  5, 21, 15, 12,\n",
       "       17, 17,  5, 12,  6, 16,  7,  4,  4,  5, 15,  4, 20,  6, 15, 11, 10,\n",
       "       12, 12,  5,  9, 11,  6,  5,  6, 16, 21,  6,  7,  5, 11,  5, 11,  5,\n",
       "        5,  5,  4, 18,  6, 20,  9, 23,  7, 12, 19, 20,  6,  9,  9, 13, 11,\n",
       "       12, 10, 16, 15,  4, 17,  7,  9, 10, 20, 15,  4,  4,  6,  5, 15,  7,\n",
       "        7, 14,  7, 15,  8,  6,  8, 10, 11,  4, 15, 13,  5, 12, 13,  9,  5,\n",
       "        4, 10, 13, 22, 12, 11, 17,  4, 16, 11, 11, 19, 19,  7, 18,  6,  8,\n",
       "        7,  7,  6, 14, 21, 14, 19, 14, 12, 12,  7, 10, 17, 18, 16, 15,  7,\n",
       "       12,  6,  9,  9, 11, 15, 15, 16, 12, 14,  9, 15,  9,  6, 10,  4, 14,\n",
       "       13, 19, 12,  7,  5, 23,  7,  8,  6,  6,  8, 17,  5,  5,  6, 12, 18,\n",
       "       19, 14, 18, 12, 13,  8,  6,  7, 15,  4, 12,  6,  7,  9,  5,  6,  6,\n",
       "        9,  6,  4,  9, 11,  6, 15, 21,  5,  5,  8,  4,  4, 15,  6, 15, 12,\n",
       "        6, 19,  4, 21,  6,  8, 15, 11, 18,  7,  4, 16, 13, 13, 22,  6, 16,\n",
       "        7, 16,  7,  8, 21,  8,  7,  8,  5,  6,  4, 13, 19,  8,  6,  7,  6,\n",
       "       14,  9, 21,  5, 19, 22, 10,  9, 11,  6, 15,  7,  4, 19, 13,  5,  7,\n",
       "       11, 13,  9,  9,  5,  4, 13,  5, 18, 12,  9,  4,  8, 10,  4,  9,  6,\n",
       "       12, 12, 14,  5,  4,  9,  8,  8, 21,  5,  4,  5, 19,  4, 15, 15, 12,\n",
       "       17, 12, 15,  6, 15,  7,  7,  7,  7, 17,  4,  8,  6, 10,  6, 13, 15,\n",
       "       13,  4,  9,  8,  7,  4, 17, 21, 14, 19,  7, 10,  9, 10, 12, 10,  2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer.tokenize(\"MKGYFGPYGGQYVPEILMGALEELEAAYEGIMKDESFWKEFNDLLRDYAGRPTPLYFARRLSEKYGARVYLKREDLLHTGAHKINNAIGQVLLAKLMGKTRIIAETGAGQHGVATATAAALFGMECVIYMGEEDTIRQKLNVERMKLLGAKVVPVKSGSRTLKDAIDEALRDWITNLQTTYYVFGSVVGPHPYPIIVRNFQKVIGEETKKQIPEKEGRLPDYIVACVSGGSNAAGIFYPFIDSGVKLIGVEAGGEGLETGKHAASLLKGKIGYLHGSKTFVLQDDWGQVQVSHSVSAGLDYSGVGPEHAYWRETGKVLYDAVTDEEALDAFIELSRLEGIIPALESSHALAYLKKINIKGKVVVVNLSGRGDKDLESVLNHPYVRERIR\")\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MKGYFGPYGGQYVPEILMGALEELEAAYEGIMKDESFWKEFNDLLRDYAGRPTPLYFARRLSEKYGARVYLKREDLLHTGAHKINNAIGQVLLAKLMGKTRIIAETGAGQHGVATATAAALFGMECVIYMGEEDTIRQKLNVERMKLLGAKVVPVKSGSRTLKDAIDEALRDWITNLQTTYYVFGSVVGPHPYPIIVRNFQKVIGEETKKQIPEKEGRLPDYIVACVSGGSNAAGIFYPFIDSGVKLIGVEAGGEGLETGKHAASLLKGKIGYLHGSKTFVLQDDWGQVQVSHSVSAGLDYSGVGPEHAYWRETGKVLYDAVTDEEALDAFIELSRLEGIIPALESSHALAYLKKINIKGKVVVVNLSGRGDKDLESVLNHPYVRERIR'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = tokenizer.untokenize(tokenized)\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-guidance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
