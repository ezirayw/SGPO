name: d3pm_finetune
model:
  _target_: models.pretraining.model.d3pm_evodiff.ByteNetDiffusion

  model_config:
    name: d3pm_evodiff
    _lambda: 0 #reweighting term for the loss function
    mask: uniform #could also support blosum or absorbing
    device: cuda
    pretrained_evodiff_ckpt: D3PM_UNIFORM_38M

  network:
  tokenizer:

  optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-4

  lr_scheduler:
    _target_:  transformers.get_linear_schedule_with_warmup
    num_warmup_steps: 10 #could play around with this
    num_training_steps: 10 #${max_epochs}, check this

train: 
  seed: 42
  gradient_clip: 10.0 #update this or get rid of it
  min_epochs: 5
  max_epochs: 5
  early_stop_patience: 0 #need this?
  batch_size: 64
  val_interval: 1.0 #per epoch
  log_interval: 10 #per step
  workers: 4
  ngpu: 1 #must agree with device

data:
  dataset_suffix: MSA
  splits: [train, validation]
  
# hydra:
#   run:
#     dir: ./outputs/${now:%Y-%m-%d_%H-%M-%S}
